---
title  : El Clasificador Multicapa
slug   : el-clasificador-multicapa
author : Javier Vélez
date   : Mayo 2024
---

## Introducción

El modelo de neurona artificial propuesto por McCulloch y Pitts en el año 1943 había supuesto un paso fundamental para arrancar una nueva forma de entender la Inteligencia Artificial. Una que miraba mimetizar las estructuras biológicas del cerebro humano formado por unidades de procesamiento interconectadas. La neurona articifial de estos autores, se presentaba así como un elemento de procesamiento de información capaz de realizar operaciones lógicas básicas. Y es que esta conceptualización, aunque abstracta, demostraría años después que sería posible simular computacionalmente los procesos básicos del pensamiento humano.

En concreto, fue Frank Rosenblatt quien, 15 años después, haría una reinterpretación del modelo teorico de neurona que resultaría fundamental para su aplicación práctica. Este autor reformularía la neurona artificial como un clasificador lineal operando sobre un espacio multidimensional. Ahora el modelo original de neurona artificial se había convertido en un agente inteligente sencillo pero eficaz a la hora de abordar problemas de clasificación y categorizar datos en diversas clases, abriendo un abanico de posibilidades en áreas como el reconocimiento de patrones y la toma de decisiones automatizada.

Una de las características más atractivas de este modelo era su capacidad de autoaprendizaje. Mediante un algoritmo de ajuste de pesos sinápticos, el modelo podía mejorar su rendimiento de clasificación con la experiencia, adaptándose a los datos de entrada sin necesidad de una programación explícita lo que supuso un importante cambio paradigmático en la forma de afrontar el diseño de sistemas inteligentes.

Sin embargo, pronto se hizo evidente que el uso de neuronas operando de forma aislada tenía limitaciones significativas. Si bien el modelo de neurona, como clasificador geométrico, podía resolver problemas de división del espacio multidimensional de datos de forma lineal, su capacidad para abordar relaciones no lineales en los datos era muy restringida. En problemas reales, donde los datos suelen presentar una complejidad inherente mucho más elevada, el modelo de neurona se mostraba insuficiente.

Surgiría, de esta forma, la necesidad de desarrollar modelos de solución más potentes capaces de capturar y clasificar patrones no lineales. Así nació el Clasificador Multicapa que exploraremos en detalle a lo largo de este artículo.

## El Clasificador Multicapa

El modelo de neurona artificial, en su concepción original, había demostrado ser un formalismo teórico eficaz para representar las operaciones básicas del procesamiento de información. Su capacidad para simular funciones lógicas y realizar cálculos sencillos abriría un nuevo horizonte en la búsqueda de modelos computacionales que imitaran la inteligencia humana.

Sin embargo, la reinterpretación que Rosenblatt haría años después, al traducir este modelo teórico en un clasificador lineal geométrico, representó un avance significativo. El autor mostraba la capacidad que presentaba el uso de una sola neurona artificial para dividir el espacio multidimensional de datos de forma lineal. Esta reformulación supondría una nueva aproximación eficaz para abordar problemas de clasificación. Pero además, su capacidad de aprendizaje automático, dirigido por los datos de entrenamiento, lo convertía en un sistema adaptable y capaz de mejorar con la experiencia.

Rosenblatt había convertido el modelo teórico de neurona artificial en un agente operativo dentro del campo de la ingeniería. Sin embargo, a pesar de estos avances, el alcance de una sola neurona, operando en aislamiento sobre un espacio de datos multidimensional, resultaba insuficiente para abordar problemas de mayor complejidad. Muchas situaciones del mundo real implicaban relaciones no lineales entre los datos, que no podían ser separadas por una simple estrategia de carácter lineal lo que supondría una limitación que conduciría a la búsqueda de nuevas arquitecturas.

En efecto, se requerían nuevas aproximaciones para resolver problemas de clasificación más complejos que trascendieran las fronteras de la linealidad. La capacidad de clasificar datos con fronteras de decisión no lineales era un objetivo fundamental para el desarrollo de la Inteligencia Artificial Conexionista de aquellos años. Esto permitiría abordar una gama mucho más amplia de problemas prácticos.

Una posible solución pudiera haber sido crear neuronas más complejas de caracter no lineal. Sin embargo, ese parecía no ser el camino. Se necesitaba una solución más elegante y escalable. La idea vino de la mano de Minsky y Paper, quienes en el año 1969 apuntarían a combinar la capacidad operacional de varias neuronas conectadas en una topología de red estratégicamente diseñada. De esta manera, múltiples neuronas, operarían sobre el espacio de datos para arrojar una señal de salida que contrinuiría, de alguna forma, a la respuesta final de la red.

Dentro de esta red, las neuronas se organizarían en capas conectadas entre sí, donde la salida de una capa sirve como entrada para la siguiente capa. Cada capa tendría una función específica en el proceso de clasificación. Y cada neurona dentro de cada capa tendría un alcance de operación local dentro del espacio total de datos. En concreto, de acuerdo a esta disposición sería posible distinguir entre los tres tipos de capas siguientes.

- **Capa de Entrada.** Esta capa sería la  responsable de recibir los datos de entrada del problema. En este sentido, actúa como el punto de partida del procesamiento de la información en el clasificador multicapa. En particular, cada neurona dentro de esta capa se corresponde con una característica o atributo de los datos de entrada. Sin embargo, esta capa no realiza ninguna transformación compleja de los datos, sino que simplemente los introduce en la red para que las capas subsiguientes puedan extraer patrones y relaciones significativas.

- **Capas Ocultas.** Las capas intermedias o capas ocultas son las que se encuentran entre la primera y la última capa de la arquitectura de red neuronal y son las responsables de realizar las transformaciones no lineales de los datos. En este sentido, pueden considerarse el eje central del clasificador multicapa, ya que sobre ellas descansan los procesos de aprendizaje de las representaciones abstractas y complejas de los datos de entrada. A diferencia de la capa de entrada, las neuronas en estas capas aplican funciones de activación no lineales a las combinaciones lineales de las entradas ponderadas que reciben. Estas funciones de activación no lineal permiten al modelo capturar relaciones no lineales en los datos, lo cual es esencial para resolver problemas complejos. El número de capas ocultas y el número de neuronas en cada capa son parámetros de diseño que se pueden ajustar para optimizar el rendimiento del modelo. Cada capa oculta sucesiva aprende representaciones de mayor nivel de abstracción, extrayendo características cada vez más sofisticadas de los datos originales.

- **Capa de Salida.**  Finalmente, la capa de salida es responsable de producir el resultado final de la clasificación. Esta capa tiene la función de generar la predicción o clasificación final del modelo. El número de neuronas en la capa de salida depende del tipo de problema que se esté resolviendo. En un problema de clasificación binaria, donde sólo hay dos clases posibles, la capa de salida típicamente tendrá una sola neurona que produce una probabilidad de pertenencia a una de las clases. Por el contrario, en un problema de clasificación múltiple, donde existen más de dos clases definidas, la capa de salida tendrá una neurona por cada clase, y cada neurona producirá una puntuación o probabilidad para esa clase.

Imaginado a diez mil pies de altura, una arquitectura de red neuronal profunda, como la que acabamos de describir, presenta diversas hileras donde se ubican neuronas artificiales conectadas entre si. Los datos fluyen desde las neuronas de la capa de entrada a cada una de las neuronas de las capas ocultas de acuerdo al orden en que éstas se encuentran dispuestas hasta alcanzar la capa final de salida cuyas neuronas se encargan de emitir un resultado. En todo este entramado, los procesos de aprendizaje se articulan sobre la base del reajuste de los pesos sinápticos que existen entre cada par de capas adyacentes.

Desde un punto de vista operativo, es posible afirmar que este tipo de redes habilita al modelo sistemico global asi creado a aprender representaciones complejas de los datos. Si las neuronas de la primera capa permiten reconocer patrones elementales sobre los datos de entrada, las neuronas de la capa subsiguiente consiguen aprender representaciones más elaboradas y complejas en base al trabajo parcial arrojado como respuesta por las neuronas de la capa anterior en la arquitectura. Esta disposición permite adquirir un comportamiento inteligente impulsado por la disposición jerarquica de las neuronas y confiere a cada neurona dentro de la red un espacio de responsabilidad diferente.

## El Clasificador Multicapa en Acción

Como ya hemos explicado, la simulación de operaciones lógicas sencillas, como la conjunción y disyunción, se habia mostrado fácilmente abordable con los modelos clásicos de neuronas artificiales. A la postre, estas operaciones, que pueden representarse mediante funciones lineales, encajaban perfectamente con la capacidad del clasificador geométrico de Rosemblatt. 

Sin embargo, para otras operaciones lógicas mínimamente más elaboradas, como lo es la disyunción exclusiva, parecían no haber  solución con el modelo de única neurona. Esta limitación impulsó la búsqueda de modelos de clasificación no lineales más potentes, capaces de abordar problemas más complejos. Y es que muchos problemas de comportamiento inteligente se revelaron como problemas de clasificación que requerían modelos no lineales más elaborados. Desde el reconocimiento de objetos en imágenes hasta la comprensión del lenguaje natural, la mayoría de las tareas de inteligencia artificial implican la clasificación de datos en categorías complejas y no linealmente separables.

Por eso, la idea de crear arquitecturas de red neuronales donde cada una de las neuronas desempeñara una labor de clasificación local y contribución combinada de acuerdo a una distribución jerarquica de las responsabilidades sobre el espacio de datos parecía francamente brillante y eficaz.

Por un lado, se lograría crear soluciones más potentes que escapaban de los límites fronterizos de la linealidad ofreciendo modelos de clasificación y comportamiento más complejos y elaborados según se requeria en los escenarios propios del mundo real.

Por otro, todo ello se conseguía sin crear un nuevo modelo de neurona más capaz. Se trataba solamente de crear estructuras de conexión compositiva estrategicamente diseñadas en forma de arquitecturas de capas donde tendría lugar un aprendizaje progresivamente más complejo de patrones reconocidos sobre los datos de entrada, a medida que profundizabamos en las capas ocultas de la red.

Este nuevo tipo de arquitectura, que aprovechaba el caracter distribuido y compositivo de familias de redes neuronales dispuestas por capas y conectadas en red resultaría verdaderamente prometedor. La nueva solución permitía al modelo aprender funcionesde clasificación complejas y no lineales al descomponer el problema en subproblemas más simples, resueltos por cada capa de neuronas. La arquitectura en capas proporcionaría así una mayor flexibilidad y capacidad incomparable con la capacidad del modelo de Rosemblatt. Tras esta nueva ideación la comunidad científica estaba preparada para dirigir su atención hacia la resolución de problemas de clasificación más complejos. A continuación, describimos algunos ejemplos de escenarios de aplicación propios del mundo real.

- **Reconocimiento de voz.** Dependiendo de las variaciones en el tono, el acento y la velocidad del habla hacen que la clasificación lineal sea insuficiente. La señal de voz humana varía por el tono, acento y velocidad, haciendo su representación no lineal. Un clasificador lineal no puede trazar límites precisos entre fonemas o palabras en este espacio complejo. Los modelos multicapa, al aprender funciones no lineales, modelan estas complejidades y mejoran la transcripción.

- **Diagnóstico médico.** Se requiere clasificar el cuadro clínico de los pacientes a partir de síntomas donde las interacciones complejas entre diferentes factores pueden no seguir patrones lineales. En el diagnóstico médico, las interacciones complejas entre síntomas y factores como la edad o el historial del paciente no son lineales. Un síntoma rara vez indica una sola enfermedad, y las enfermedades tienen múltiples síntomas. Los modelos multicapa aprenden estas relaciones intrincadas, ayudando a los médicos a realizar diagnósticos más precisos.

- **Conducción autónoma**. La detección de objetos y la toma de decisiones en entornos dinámicos requiere la capacidad de clasificar escenas complejas. La conducción autónoma requiere clasificar escenas complejas a partir de datos de sensores como cámaras y lidars. Detectar objetos en este flujo de datos es no lineal, ya que las relaciones entre píxeles o puntos lidar no son lineales. Los modelos multicapa aprenden estas relaciones y permiten a los vehículos tomar decisiones seguras.

- **Procesamiento del lenguaje natural.** La comprensión del significado y el contexto de las palabras a menudo implica relaciones no lineales. En el procesamiento del lenguaje natural, el significado de las palabras depende del contexto, lo que implica relaciones no lineales. Los modelos de procesamiento de lenguaje deben capturar estas relaciones para tareas como la traducción y el análisis de sentimientos. Los modelos multicapa son eficaces en procesamiento de lenguaje debido a su capacidad para aprender representaciones no lineales del lenguaje.

La arquitectura asi alcanzada, basada en capas de neuronas interconectadas, había proporcionado la capacidad de aprender representaciones jerárquicas de los datos. En efecto, cada capa extraría características progresivamente más abstractas, permitiendo al modelo capturar las intrincadas relaciones no lineales presentes en dominios como los que acabamos de citar. Si bien el modelo de clasificación geométrica original demostró ser insuficiente, esta extensión multicapa representaría un avance fundamental, sentando las bases para el desarrollo de arquitecturas de redes neuronales más profundas y poderosas capaces de afrontar los retos de la inteligencia artificial moderna.

## Conclusiones

Si algo había consolidado el tienmpo es que el modelo de neurona simple y la reinterpretación de Rosenblatt como clasificador geométrico habían resultado insuficintes ante la complejidad de muchos problemas del mundo real. La limitación para abordar clasificaciones no lineales se habia convertido en un obstáculo para el desarrollo de sistemas de Inteligencia Artificial más avanzados.

Aunque la contribución que Minsky y Papert de 1969, fue inicialmente presentada como una crítica a las limitaciones sobre el modelo de clasificador geométrico de Rosemblatt, este trabajo pronto sentaría las bases de una nueva solución que abriría el camino de la Inteligencia Artificial Conexionista. 

La propuesta de corrección era no tanto mirar hacia modelos de neurona más complejos sino combinar estratégicamente múltiples neuronas, operando como clasificadores locales en red y dispuestas por capas. Esta arquitectura en capas permitiría al modelo aprender representaciones jerárquicas de los datos, capturando patrones complejos y no lineales.

A partir de entonces, todas las soluciones dentro de la  Inteligencia Artificial Conexionista se diseñarían de acuerdo con topologías de arquitecturas de red profundas como se proponía en el clasificador multicapa. Esta disposición modular y distribuida permitió construir sistemas de inteligencia artificial capaces de abordar problemas de una complejidad sin precedentes.
